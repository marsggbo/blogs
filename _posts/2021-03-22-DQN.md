# 强化学习2-动作价值函数&DQN

## 回顾动作价值函数

之前的笔记[强化学习1-基础概念(state,action,reward,policy)](https://zhuanlan.zhihu.com/p/358787399)介绍了强化学习的基础概念以及动作价值函数，这里再回顾一下：


- **Discounted return**： $U_{t}=R_{t}+\gamma R_{t+1}+\gamma^{2} R_{t+2}+\gamma^{3} R_{t+3}+\cdots$
- 策略$\pi$的动作价值函数：$Q_{\pi}\left(s_{t}, a_{t}\right)=\mathbb{E}\left[U_{t} \mid S_{t}=s_{t}, A_{t}=a_{t}\right]$
- 最优动作价值函数：$Q^{\star}\left(s_{t}, a_{t}\right)=\max _{\pi} Q_{\pi}\left(s_{t}, a_{t}\right)$

如果我们知道$Q^{\star}$，我们就能知道任意时刻$s$应该执行的操作$a^{\star}=\underset{a}{\operatorname{argmax}} Q^{\star}(s, a)$

但是我们是不知道$Q^{\star}$的，所以一种解决办法是使用Deep Q-Netwrok (DQN)，即使用一个神经网络$Q(s, a ; \mathbf{w})$去近似$Q(s, a)$，下面介绍DQN原理。


## Deep Q-Netwrok (DQN)

![DQN](https://raw.githubusercontent.com/marsggbo/PicBed/master/marsggbo/2021_3_22_1616421224832.png)


DQN的示意图如上，state $s_t$作为DQN的输入，其输出是对每个action的价值预测，比如left, right和up的价值分别预测2000,1000和3000。



![DQN](https://raw.githubusercontent.com/marsggbo/PicBed/master/marsggbo/2021_3_22_1616421354370.png)

上图展示了DQN的执行过程，可以看到每次都是选择价值最大的动作，之后再下一个状态继续预测并执行价值最大的动作。

这个执行逻辑很好理解，但是这与传统的监督任务不同，我们没有真实的label，那DQN该如何更新呢？为了解决这个问题，我们需要用到 **Temporal Difference (TD)**。为方便理解，我们首先看一个例子来帮助理解TD。

### Temporal Difference (TD)



![Temporal Difference](https://raw.githubusercontent.com/marsggbo/PicBed/master/marsggbo/2021_3_22_1616422543161.png)

假设我们现在需要从纽约（NYC）开车到亚特兰大（Atlanta），我们想用DQN来预测这个路程要花的时间。

- 假设一开始模型预测要花费$q=1000$分钟。（这个值很可能只是模型瞎猜的，后面我们需要更新模型参数来使模型预测更精准）
- 假设我们真实地从NYC开车到Atlanta后，实测时间是$y=860$分钟。
- 这个实测时间860分钟其实就可以当做是真实的label，所以我们可以求出$loss=\frac{1}{2}(q-y)^2$
- 通过上面的loss我们就可以进一步更新DQN


上面的例子中如果要更新DQN需要我们真实地开车从NYC到Atlanta，但是也许我们并不能真的开车到Atlanta，很可能开到华盛顿的时候就没法继续前进了（比如车坏了，路被封了等），那这个时候岂不是就没法更新DQN了？但是我们要相信，只要不放弃，方法总比困难多，即使中途停止，我们也可以利用这个信息来更新DQN。

![TD learning](https://raw.githubusercontent.com/marsggbo/PicBed/master/marsggbo/2021_3_22_1616423262469.png)

如上图示，假如我们开车到了DC后就没法继续前进了，但是NYC到DC之间实际消耗时间我们知道是300分钟，并且我们继续利用DQN预测DC到Atlanta的时间是600分钟。此时可以知道，这个时候预测的NYC到Atlanta的时间变成了300+600=900，这和最开始预测的1000分钟有了出入。

尽管预测的600也不一定准确，但是300是真实的数据，那300+600我们认为要比最开始预测的1000肯定要准确一些，所以我们可以把300+600看作是真实的label，即**TD target：y=300+600=900**。

综上，我们可以通过TD target **y**和最开始的预测值$Q(W)$计算得到$L=\frac{1}{2}(Q(\mathbf{w})-y)^{2}$，同样通过梯度下降更新DQN。

类似于TD target，还有一个概念是**TD error**。什么意思呢？还是前面的例子，DQN一开始预测NYC到Atlanta的时间是1000分钟，然后从NYC到了DC又预测 DC到Atlanta的时间是600分钟，那么换句话说，他预测NYC到DC的时间是400分钟，而真实的时间是300分钟，那么**TD error $\delta$=400-300=100**

![TD error](https://raw.githubusercontent.com/marsggbo/PicBed/master/marsggbo/2021_3_22_1616424216594.png)


### 将TD learning应用到DQN


![Apply TD to DQN](https://raw.githubusercontent.com/marsggbo/PicBed/master/marsggbo/2021_3_22_1616424439043.png)


在上面的开车时间预测例子中，我们是希望下面等式两边能尽可能地接近，方法是通过二者之间组成的loss来更新DQN的权重

$$
T_{\mathrm{NYC} \rightarrow \mathrm{ATL}} \approx T_{\mathrm{NYC} \rightarrow \mathrm{DC}}+T_{\mathrm{DC} \rightarrow \mathrm{ATL}}
$$

那么对应到深度强化学习，就是如下等式
$$
Q\left(s_{t}, a_{t} ; \mathbf{w}\right) \approx r_{t}+\gamma \cdot Q\left(s_{t+1}, a_{t+1} ; \mathbf{w}\right) \tag{1}
$$

你可能会问为什么上面式子右边要乘上一个因子$\gamma$呢？这里我们就需要再次回顾一下价值函数$Q$的定义了。

首先我们可以得到下式
$$
\begin{aligned}
U_{t} &=R_{t}+\gamma \cdot R_{t+1}+\gamma^{2} \cdot R_{t+2}+\gamma^{3} \cdot R_{t+3}+\gamma^{4} \cdot R_{t+4}+\cdots \\
&=R_{t}+\gamma \cdot\left(R_{t+1}+\gamma \cdot R_{t+2}+\gamma^{2} \cdot R_{t+3}+\gamma^{3} \cdot R_{t+4}+\cdots\right)
\end{aligned}\tag{2}
$$
进而知道
$$
U_{t}=R_{t}+\gamma \cdot U_{t+1}\tag{3}
$$

而$Q(t)=E[U_t]$,所以

$$
Q\left(s_{t}, a_{t} ; \mathbf{w}\right) = \mathbb{E}\left[R_{t}\right]+\gamma \cdot Q\left(S_{t+1}, A_{t+1} ; \mathbf{w}\right) \tag{4}
$$

注意比对公式1和4，我们可以看到区别是前者是$r_t$,后者是$E[R_t]$。因为我们可能没法精准地求出$E[R_t]$，所以每次只用一个时刻的奖励来近似计算。

总结起来：
- 预测的value是：$Q\left(s_{t}, a_{t} ; \mathbf{w}_{t}\right)$
- **TD target**：
$$
\begin{aligned}
y_{t} &=r_{t}+\gamma \cdot Q\left(s_{t+1}, a_{t+1} ; \mathbf{w}_{t}\right) \\
&=r_{t}+\gamma \cdot \max _{a} Q\left(s_{t+1}, a ; \mathbf{w}_{t}\right)
\end{aligned}
$$
- 那么loss就是
$$
L_{t}=\frac{1}{2}\left[Q\left(s_{t}, a_{t} ; \mathbf{w}\right)-y_{t}\right]^{2}
$$
- 最后更新梯度
$$
\mathbf{w}_{t+1}=\mathbf{w}_{t}-\left.\alpha \cdot \frac{\partial L_{t}}{\partial \mathbf{w}}\right|_{\mathbf{w}=\mathbf{w}_{t}}
$$


## 总结

![Value-based RL](https://raw.githubusercontent.com/marsggbo/PicBed/master/marsggbo/2021_3_22_1616425470613.png)

![TD](https://raw.githubusercontent.com/marsggbo/PicBed/master/marsggbo/2021_3_22_1616425485926.png)

<footer style="color:white;;background-color:rgb(24,24,24);padding:10px;border-radius:10px;">
<h3 style="text-align:center;color:tomato;font-size:16px;" id="autoid-2-0-0">
<center>
<span>微信公众号：AutoML机器学习</span><br>
<img src="https://pic4.zhimg.com/80/v2-87083e55cd41dbef83cc840c142df48a_720w.jpeg" style="width:200px;height:200px">
</center>
<b>MARSGGBO</b><b style="color:white;"><span style="font-size:25px;">♥</span>原创</b><br>
<span>如有意合作或学术讨论欢迎私戳联系~<br>邮箱:marsggbo@foxmail.com</span>
<b style="color:white;"><br>
2021-03-22 21:06:04  <p></p>
</b><p><b style="color:white;"></b>
</p></h3>
</footer>
